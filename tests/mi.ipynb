{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0901c328",
   "metadata": {},
   "source": [
    "# Mutual Information (MI)\n",
    "\n",
    "Saleh Rezaeiravesh, saleh.rezaeiravesh@manchester.ac.uk\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9781adee",
   "metadata": {},
   "source": [
    "## Mutual Information (MI)\n",
    "\n",
    "[Shannon 1948](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf): MI can be used to quantify the overlap of information content of two systems/variables.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea5f157",
   "metadata": {},
   "source": [
    "### MI for Random Variables (RVs)\n",
    "Consider two continuous **random variables** $X$ and $Y$, for which we have $n$ jointly observed $(x_i,y_i)$. If the joint PDF of $X$ and $Y$ is $f(x,y)$, then the mutual information between $X$ and $Y$ is, \n",
    "\n",
    "$$\n",
    "I(X,Y) =\\int\\int p(x,y)\\ln( \\frac{p(x,y)}{p_x(x)p_y(y)}) dx dy\n",
    "$$\n",
    "\n",
    "### MI for Time Series (TS)\n",
    "* In contrast for uncorrelated RVs $X, Y$, where the mutual information is a symmetric quantity, for $x(t)$ and $y(t)$, the MI can be **asymmetric**. \n",
    "\n",
    "* MI measure **nonlinear** dependency between time series or RVs.\n",
    "\n",
    "* According to [Schreiber, 2000](https://arxiv.org/pdf/nlin/0001042), the MI at lag $\\tau$ for two time series $x_t$ and $y_t$ is defined as, \n",
    "\n",
    "$$\n",
    "M_{xy}(\\tau) = - \\sum_{k} p(x_k,y_{k-\\tau}) \\log\\frac{p(x_k,y_{k-\\tau})}{p(x_k)p(y_k)}\n",
    "$$\n",
    "\n",
    "\n",
    "**Note:** Mutual information (MI) is still a symmetric metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596c1e64",
   "metadata": {},
   "source": [
    "## Methods for Estimating MI\n",
    "\n",
    "### KDE/Binning methods\n",
    "Our aim is to esimtate $\\hat{I}(X,Y)$ from a set of finite observed samples $z_i=(x_i,y_i)$, $i=1,2,\\cdots,n$.\n",
    "\n",
    "In many text books, e.g. [this one](http://staff.ustc.edu.cn/~cgong821/Wiley.Interscience.Elements.of.Information.Theory.Jul.2006.eBook-DDU.pdf), the **mutual information (MI)** is written as, \n",
    "\n",
    "$$\n",
    "I(X,Y) = H(X) + H(Y) - H(X,Y)\n",
    "$$\n",
    "\n",
    "\n",
    "where, \n",
    "* $H(X)$ and $H(Y)$ are the **marginal entropies**, and\n",
    "* $H(X,Y)$ is the **joint entropy**\n",
    "\n",
    "The above three entropy can be estimated using KDE or Binning methods, for both RVs and TS. \n",
    "\n",
    "For time series, the above expression reads as, \n",
    "\n",
    "$$\n",
    "I(x_k,y_{k-\\tau}) = H(x_k) + H(y_k) - H(x_k,y_{k-\\tau})\n",
    "$$\n",
    "\n",
    "We can extend the KDE/Binning methods to compute $I(X,Y)$ by:\n",
    "1. Estimate the marginal PDFs of $X$ and $Y$\n",
    "2. Estimate the joint PDF of $X$ and $Y$\n",
    "3. Numerically compute the above double integral\n",
    "\n",
    "### KSG (KL/KNN-based) method:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf71fde7",
   "metadata": {},
   "source": [
    "As detailed in [Kraskov-Stogbauer-Grassberger (KSG), 2004](https://arxiv.org/pdf/cond-mat/0305641), the error involved in estimation of each separate $H$ using KL/KNN method are not cencelled by each others' and would propagate into the estimated $I(X,y)$:\n",
    "\n",
    "$$\n",
    "\\hat{H}(X) = -\\psi(k)+\\psi(N)+\\ln c_{d_x} +\\frac{d_x}{n}\\sum_{i=1}^n \\ln \\epsilon_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{H}(Y) = -\\psi(k)+\\psi(N)+\\ln c_{d_y} +\\frac{d_y}{n}\\sum_{i=1}^n \\ln \\epsilon_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{H}(X,Y) = -\\psi(k)+\\psi(N)+\\ln (c_{d_x}c_{d_y}) +\\frac{d_x+d_y}{n}\\sum_{i=1}^n \\ln \\epsilon_i\n",
    "$$\n",
    "\n",
    "This reference proposed alternative approach based on the KL/KNN estimator. We call this **KSG Estimator**. They proposed two estimators, where we use the 1st one. \n",
    "\n",
    "$$\n",
    "I^{(1)}(X,Y) = \\psi(k) + \\psi(n) - \\langle \\psi(n_x)+\\psi(n_y)\\rangle\n",
    "$$\n",
    "\n",
    "$$\n",
    "I^{(2)}(X,Y) = \\psi(k) + \\psi(n) - 1/k - \\langle \\psi(n_x+1)+\\psi(n_y+1)\\rangle\n",
    "$$\n",
    "\n",
    "The above expressions can be evaluated by the KNN method, for both RVs and time series. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b3476c",
   "metadata": {},
   "source": [
    "## Validation of MI estimators for RVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94de35c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fad9760",
   "metadata": {},
   "source": [
    "## Validation of MI estimators for time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03d9b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
